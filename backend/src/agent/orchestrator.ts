// @ts-nocheck
/**
 * Agent Orchestrator
 * Main agent loop that coordinates thinking LLM and tool execution
 */

import { getGenAIClient, extractText, extractFunctionCalls, extractThinking, safetySettings } from '../lib/genai.js';
import { config } from '../config/index.js';
import { AGENT_TOOLS, executeTool, type ToolContext } from './tools/index.js';
import { AGENT_SYSTEM_PROMPT } from './prompts/system.js';
import { createLogger } from '../lib/logger.js';

const logger = createLogger('agent');

// ============================================
// Types
// ============================================

export interface AgentInput {
  userId: string;
  conversationId: string;
  message: {
    text?: string;
    images?: Array<{ id: string; data: string; mimeType: string }>;
  };
  conversationHistory: ConversationMessage[];
}

export interface ConversationMessage {
  role: 'user' | 'assistant';
  content: {
    text?: string;
    images?: Array<{ id: string; data: string; mimeType: string }>;
    generatedImages?: Array<{ id: string; url: string }>;
  };
}

export interface AgentOutput {
  response: {
    text: string;
    generatedImages?: Array<{ id: string; url: string; thumbnailUrl?: string }>;
    guiRequest?: {
      type: string;
      message?: string;
      prefillData?: Record<string, any>;
    };
  };
  toolCalls: ToolCallRecord[];
  thinking?: string;
}

export interface ToolCallRecord {
  tool: string;
  arguments: Record<string, any>;
  result: any;
  timestamp: Date;
}

interface AgentContext {
  messages: any[];
  imageContext: Record<string, string>;
}

// ============================================
// Constants
// ============================================

const MAX_ITERATIONS = 5;
const THINKING_MODEL = config.ai.models.thinking;

// å·¥å…·åç§°æ˜¾ç¤ºæ˜ å°„
function getToolDisplayName(toolName: string): string {
  const nameMap: Record<string, string> = {
    'stylist': 'æ­é…å¸ˆ',
    'analyze_image': 'å›¾åƒåˆ†æ',
    'generate_model_image': 'ç”Ÿæˆæ¨¡ç‰¹å›¾',
    'change_outfit': 'æ¢æ­é…',
    'change_model': 'æ¢æ¨¡ç‰¹',
    'replicate_reference': 'å¤åˆ»å‚è€ƒå›¾',
    'edit_image': 'ç¼–è¾‘å›¾ç‰‡',
  };
  return nameMap[toolName] || toolName;
}

// ============================================
// Main Agent Function
// ============================================

/**
 * Run the agent loop
 * 1. Build context from conversation history and current message
 * 2. Call thinking LLM with tools
 * 3. If tool call: execute tool, append result, continue loop
 * 4. If no tool call: return final response
 */
export async function runAgent(input: AgentInput): Promise<AgentOutput> {
  const client = getGenAIClient();
  const toolCalls: ToolCallRecord[] = [];
  let thinking = '';
  
  logger.info('Starting agent run', { 
    userId: input.userId, 
    conversationId: input.conversationId,
    hasText: !!input.message.text,
    imageCount: input.message.images?.length || 0,
  });
  
  // Build initial context
  let context = buildContext(input);
  
  // Agent Loop
  for (let iteration = 0; iteration < MAX_ITERATIONS; iteration++) {
    logger.debug(`Agent iteration ${iteration + 1}/${MAX_ITERATIONS}`);
    
    try {
      // Call Thinking LLM
      logger.info(`Calling LLM model: ${THINKING_MODEL}`);
      const llmStartTime = Date.now();
      
      const response = await client.models.generateContent({
        model: THINKING_MODEL,
        contents: [
          { role: 'user', parts: [{ text: AGENT_SYSTEM_PROMPT }] },
          { role: 'model', parts: [{ text: 'æˆ‘ç†è§£äº†ï¼Œæˆ‘æ˜¯ Onstage çš„ AI åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·ç”Ÿæˆæœé¥°è¥é”€å†…å®¹ã€‚æˆ‘ä¼šæ ¹æ®ç”¨æˆ·çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚' }] },
          ...context.messages,
        ],
        config: {
          tools: [{ functionDeclarations: AGENT_TOOLS as any }],
          safetySettings,
        },
      });
      
      logger.info(`LLM response received`, { duration: Date.now() - llmStartTime });
      
      const candidate = response.candidates?.[0];
      if (!candidate) {
        throw new Error('No response from agent');
      }
      
      // Extract thinking process (if model supports it)
      const iterationThinking = extractThinking(response);
      if (iterationThinking) {
        thinking += iterationThinking + '\n';
      }
      
      // Check for function calls
      const functionCalls = extractFunctionCalls(response);
      
      if (functionCalls.length > 0) {
        const functionCall = functionCalls[0]; // Handle one at a time
        
        logger.info('Tool call detected', { tool: functionCall.name, args: functionCall.args });
        
        // æå–æ¨¡å‹å“åº”çš„å®Œæ•´ partsï¼ˆåŒ…å« thought å’Œ thought_signatureï¼‰
        const modelParts = candidate.content?.parts || [];
        
        // è°ƒè¯•æ—¥å¿—ï¼šæŸ¥çœ‹ modelParts ç»“æ„
        logger.info('Model parts structure', {
          partsCount: modelParts.length,
          partTypes: modelParts.map((p: any) => Object.keys(p)),
          hasThought: modelParts.some((p: any) => p.thought),
          hasThoughtSignature: modelParts.some((p: any) => p.thoughtSignature),
          hasFunctionCall: modelParts.some((p: any) => p.functionCall),
        });
        
        // Execute tool
        const toolContext: ToolContext = {
          userId: input.userId,
          conversationId: input.conversationId,
          imageContext: context.imageContext,
        };
        
        try {
          const toolResult = await executeTool(functionCall.name, functionCall.args, toolContext);
          
          // Record tool call
          toolCalls.push({
            tool: functionCall.name,
            arguments: functionCall.args,
            result: toolResult,
            timestamp: new Date(),
          });
          
          // Check if this is a GUI request
          if (functionCall.name === 'request_gui_input') {
            return {
              response: {
                text: toolResult.message || 'è¯·åœ¨ä¸‹æ–¹å®Œæˆæ“ä½œ',
                guiRequest: {
                  type: functionCall.args.gui_type,
                  message: functionCall.args.message,
                  prefillData: functionCall.args.prefill_data,
                },
              },
              toolCalls,
              thinking,
            };
          }
          
          // ç®€åŒ–é€»è¾‘ï¼šå·¥å…·æ‰§è¡ŒæˆåŠŸåç›´æ¥è¿”å›ç»“æœ
          // é¿å…å¤šè½®è°ƒç”¨æ—¶çš„ thought_signature é—®é¢˜
          if (toolResult.images) {
            // å¦‚æœå·¥å…·è¿”å›äº†å›¾ç‰‡ï¼Œç›´æ¥è¿”å›
            return {
              response: {
                text: toolResult.message || 'å›¾ç‰‡å·²ç”Ÿæˆå®Œæˆ âœ¨',
                generatedImages: toolResult.images,
              },
              toolCalls,
              thinking,
            };
          }
          
          // å¦‚æœå·¥å…·æ²¡æœ‰è¿”å›å›¾ç‰‡ï¼ˆå¦‚ stylistï¼‰ï¼Œæ„å»ºå‹å¥½çš„å“åº”
          let responseText = toolResult.message || 'å¤„ç†å®Œæˆ';
          
          // ç‰¹æ®Šå¤„ç† stylist å·¥å…·çš„è¾“å‡º
          if (functionCall.name === 'stylist' && toolResult.outfit_instruct_zh) {
            responseText = `ğŸ¨ **æ­é…æ–¹æ¡ˆå·²ç”Ÿæˆ**\n\n${toolResult.outfit_instruct_zh}\n\néœ€è¦æˆ‘åŸºäºè¿™ä¸ªæ­é…æ–¹æ¡ˆç”Ÿæˆæ¨¡ç‰¹å›¾å—ï¼Ÿ`;
          }
          
          return {
            response: {
              text: responseText,
            },
            toolCalls,
            thinking,
          };
          
        } catch (toolError) {
          logger.error('Tool execution failed', { tool: functionCall.name, error: toolError });
          
          // å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œç›´æ¥è¿”å›é”™è¯¯ä¿¡æ¯
          const errorMessage = toolError instanceof Error ? toolError.message : 'Unknown error';
          
          return {
            response: {
              text: `ğŸ˜… æŠ±æ­‰ï¼Œåœ¨æ‰§è¡Œã€Œ${getToolDisplayName(functionCall.name)}ã€æ—¶é‡åˆ°äº†é—®é¢˜ï¼š${errorMessage}\n\nè¯·ç¨åé‡è¯•æˆ–æ¢ä¸€ç§æ–¹å¼æè¿°ä½ çš„éœ€æ±‚ã€‚`,
            },
            toolCalls,
            thinking,
          };
        }
      }
      
      // No tool call - extract final response
      const textResponse = extractText(response);
      
      // Check if any tool calls returned images
      const generatedImages = toolCalls
        .filter(tc => tc.result?.images)
        .flatMap(tc => tc.result.images);
      
      return {
        response: {
          text: textResponse || 'æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚è¯·é‡è¯•æˆ–æ¢ä¸€ç§æ–¹å¼æè¿°ä½ çš„éœ€æ±‚ã€‚',
          generatedImages: generatedImages.length > 0 ? generatedImages : undefined,
        },
        toolCalls,
        thinking,
      };
      
    } catch (error) {
      logger.error('Agent iteration error', { iteration, error });
      throw error;
    }
  }
  
  // Max iterations reached
  logger.warn('Agent reached max iterations');
  
  const generatedImages = toolCalls
    .filter(tc => tc.result?.images)
    .flatMap(tc => tc.result.images);
  
  return {
    response: {
      text: 'æˆ‘å·²ç»å®Œæˆäº†å¤šè½®å¤„ç†ï¼Œè¿™æ˜¯ç›®å‰çš„ç»“æœã€‚å¦‚æœéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚',
      generatedImages: generatedImages.length > 0 ? generatedImages : undefined,
    },
    toolCalls,
    thinking,
  };
}

// ============================================
// Context Building
// ============================================

/**
 * Build context from conversation history and current message
 */
function buildContext(input: AgentInput): AgentContext {
  const messages: any[] = [];
  const imageContext: Record<string, string> = {};
  
  // Add conversation history (last 10 turns)
  const recentHistory = input.conversationHistory.slice(-10);
  for (const msg of recentHistory) {
    if (msg.role === 'user') {
      const parts: any[] = [];
      
      if (msg.content.text) {
        parts.push({ text: msg.content.text });
      }
      
      if (msg.content.images) {
        for (const img of msg.content.images) {
          parts.push({
            inlineData: {
              mimeType: img.mimeType || 'image/jpeg',
              data: img.data.replace(/^data:image\/\w+;base64,/, ''),
            },
          });
        }
      }
      
      if (parts.length > 0) {
        messages.push({ role: 'user', parts });
      }
    } else if (msg.role === 'assistant') {
      messages.push({
        role: 'model',
        parts: [{ text: msg.content.text || '' }],
      });
    }
  }
  
  // Add current message
  const currentParts: any[] = [];
  
  // Process uploaded images
  if (input.message.images && input.message.images.length > 0) {
    for (let i = 0; i < input.message.images.length; i++) {
      const img = input.message.images[i];
      const imageId = `image_${i + 1}`;
      
      // Store in imageContext for tool use
      imageContext[imageId] = img.data;
      
      // Add image to message
      currentParts.push({
        inlineData: {
          mimeType: img.mimeType || 'image/jpeg',
          data: img.data.replace(/^data:image\/\w+;base64,/, ''),
        },
      });
    }
    
    // Add image labels to text
    const imageLabels = input.message.images.map((_, i) => `å›¾${i + 1}`).join('ã€');
    const imageNote = `[ç”¨æˆ·ä¸Šä¼ äº† ${input.message.images.length} å¼ å›¾ç‰‡: ${imageLabels}]`;
    currentParts.push({ text: imageNote });
  }
  
  // Add text
  if (input.message.text) {
    currentParts.push({ text: input.message.text });
  }
  
  if (currentParts.length > 0) {
    messages.push({ role: 'user', parts: currentParts });
  }
  
  return { messages, imageContext };
}

/**
 * Append tool result to context
 * @param modelParts - æ¨¡å‹å“åº”çš„å®Œæ•´ partsï¼ˆåŒ…å« thoughtã€thought_signature å’Œ functionCallï¼‰
 */
function appendToolResult(
  context: AgentContext, 
  toolName: string, 
  toolArgs: Record<string, any>,
  result: any,
  modelParts?: any[]
): AgentContext {
  // è°ƒè¯•ï¼šæ‰“å° appendToolResult æ¥æ”¶åˆ°çš„ modelParts
  logger.info('appendToolResult called', {
    toolName,
    modelPartsCount: modelParts?.length || 0,
    modelPartTypes: modelParts?.map((p: any) => Object.keys(p)) || [],
  });
  
  // æ·»åŠ æ¨¡å‹å“åº”ï¼ˆåŒ…å«å®Œæ•´çš„ parts ä»¥ä¿ç•™ thought_signatureï¼‰
  if (modelParts && modelParts.length > 0) {
    // ä½¿ç”¨æ¨¡å‹è¿”å›çš„å®Œæ•´ partsï¼ˆåŒ…å« thought, thought_signature, functionCallï¼‰
    context.messages.push({
      role: 'model',
      parts: modelParts,
    });
    logger.info('Added model parts to context', { partsCount: modelParts.length });
  } else {
    // å…œåº•ï¼šå¦‚æœæ²¡æœ‰ modelPartsï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
    logger.warn('No modelParts provided, using fallback');
    context.messages.push({
      role: 'model',
      parts: [{
        functionCall: {
          name: toolName,
          args: toolArgs,
        },
      }],
    });
  }
  
  // æ·»åŠ å‡½æ•°å“åº”
  context.messages.push({
    role: 'user',
    parts: [{
      functionResponse: {
        name: toolName,
        response: result,
      },
    }],
  });
  
  // If result contains images, add them to imageContext
  if (result.images) {
    for (let i = 0; i < result.images.length; i++) {
      const img = result.images[i];
      const imageId = `generated_${toolName}_${i + 1}`;
      if (img.data) {
        context.imageContext[imageId] = img.data;
      } else if (img.url) {
        context.imageContext[imageId] = img.url;
      }
    }
  }
  
  return context;
}



